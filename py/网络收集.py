from lxml import etree
import time
import datetime
from datetime import datetime, timedelta  # ç¡®ä¿ timedelta è¢«å¯¼å…¥
import concurrent.futures
#from selenium import webdriver
#from selenium.webdriver.chrome.options import Options
from concurrent.futures import ThreadPoolExecutor
import requests
import re
import os
import threading
from queue import Queue
import queue
from datetime import datetime
import fileinput
from tqdm import tqdm
from pypinyin import lazy_pinyin
from opencc import OpenCC
import base64
import cv2
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from translate import Translator  # å¯¼å…¥Translatorç±»,ç”¨äºæ–‡æœ¬ç¿»è¯‘
## å®šä¹‰txtæ–‡ä»¶çš„URLåˆ—è¡¨
urls = [
       'https://ghproxy.cc/https://raw.githubusercontent.com/frxz751113/IPTVzb1/refs/heads/main/%E7%BB%BC%E5%90%88%E6%BA%90.txt',
       'https://ghproxy.cc/https://raw.githubusercontent.com/frxz751113/IPTVzb1/refs/heads/main/%E7%BD%91%E7%BB%9C%E6%94%B6%E9%9B%86.txt',
       'https://ghproxy.cc/https://raw.githubusercontent.com/wcb1969/iptv/refs/heads/main/TV2025',
       '',
       '',
       '',
       '',
       '',
       '',
       '',
       '',
       ''
]
# åˆå¹¶æ–‡ä»¶çš„å‡½æ•°
def merge_txt_files(urls, output_filename='æ±‡æ€».txt'):
    try:
        with open(output_filename, 'w', encoding='utf-8') as outfile:
            for url in urls:
                try:
                    response = requests.get(url)
                    response.raise_for_status()  # ç¡®ä¿è¯·æ±‚æˆåŠŸ
                    # å°è¯•å°†å“åº”å†…å®¹è§£ç ä¸ºUTF-8ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•å…¶ä»–ç¼–ç 
                    try:
                        content = response.content.decode('utf-8')
                    except UnicodeDecodeError:
                        content = response.content.decode('gbk')  # å°è¯•GBKç¼–ç 
                    outfile.write(content + '\n')
                except requests.RequestException as e:
                    print(f'Error downloading {url}: {e}')
    except IOError as e:
        print(f'Error writing to file: {e}')

# è°ƒç”¨å‡½æ•°
merge_txt_files(urls)



#ç®€ä½“è½¬ç¹ä½“
# åˆ›å»ºä¸€ä¸ªOpenCCå¯¹è±¡,æŒ‡å®šè½¬æ¢çš„è§„åˆ™ä¸ºç¹ä½“å­—è½¬ç®€ä½“å­—
converter = OpenCC('t2s.json')#ç¹è½¬ç®€
#converter = OpenCC('s2t.json')#ç®€è½¬ç¹
# æ‰“å¼€txtæ–‡ä»¶
with open('æ±‡æ€».txt', 'r', encoding='utf-8') as file:
    traditional_text = file.read()
# è¿›è¡Œç¹ä½“å­—è½¬ç®€ä½“å­—çš„è½¬æ¢
simplified_text = converter.convert(traditional_text)
# å°†è½¬æ¢åçš„ç®€ä½“å­—å†™å…¥txtæ–‡ä»¶
with open('æ±‡æ€».txt', 'w', encoding='utf-8') as file:
    file.write(simplified_text)



with open('æ±‡æ€».txt', 'r', encoding="utf-8") as file:
    # è¯»å–æ‰€æœ‰è¡Œå¹¶å­˜å‚¨åˆ°åˆ—è¡¨ä¸­
    lines = file.readlines()
#å®šä¹‰æ›¿æ¢è§„åˆ™çš„å­—å…¸å¯¹é¢‘é“åæ›¿æ¢
replacements = {
    	"CCTV-1é«˜æ¸…æµ‹è¯•": "",
    	"CCTV-2é«˜æ¸…æµ‹è¯•": "",
    	"CCTV-7é«˜æ¸…æµ‹è¯•": "",
    	"CCTV-10é«˜æ¸…æµ‹è¯•": "",
    	"ä¸­å¤®": "CCTV",
    	"é«˜æ¸…""": "",
    	"HD": "",
    	"æ ‡æ¸…": "",
    	"amc": "AMC",
    	"CCTV1ç»¼åˆ": "CCTV1",
    	"CCTV2è´¢ç»": "CCTV2",
    	"CCTV3ç»¼è‰º": "CCTV3",
    	"å›½é™…": "",
    	"5ä½“è‚²": "5",
    	"6ç”µå½±": "6",
    	"å†›å†œ": "",
    	"8å½±è§†": "8",
    	"9çºªå½•": "9",
    	"0ç§‘æ•™": "0",
    	"2ç¤¾ä¼šä¸æ³•": "2",
    	"3æ–°é—»": "3",
    	"4å°‘å„¿": "4",
    	"5éŸ³ä¹": "5",
    	"": "",
    	"": "",
    	"": "",
    	"": "",
    	"": "",
    	"": "",
    	"å’ªå’•": "",
    	"": "",
    	"è¶…æ¸…": "",
    	"é¢‘é“": "",
    	"CCTV-": "CCTV",
    	"CCTV_": "CCTV",
    	" ": "",
    	"CCTVé£äº‘å‰§åœº": "é£äº‘å‰§åœº",
    	"CCTVç¬¬ä¸€å‰§åœº": "ç¬¬ä¸€å‰§åœº",
    	"CCTVæ€€æ—§å‰§åœº": "æ€€æ—§å‰§åœº",
    	"ç†ŠçŒ«å½±é™¢": "ç†ŠçŒ«ç”µå½±",
    	"ç†ŠçŒ«çˆ±ç”Ÿæ´»": "ç†ŠçŒ«ç”Ÿæ´»",
    	"çˆ±å® å® ç‰©": "å® ç‰©ç”Ÿæ´»",
    	"[ipv6]": "",
    	"ä¸“åŒº": "",
    	"å«è§†è¶…": "å«è§†",
    	"CCTVé£äº‘å‰§åœº": "é£äº‘å‰§åœº",
    	"CCTVç¬¬ä¸€å‰§åœº": "ç¬¬ä¸€å‰§åœº",
    	"CCTVæ€€æ—§å‰§åœº": "æ€€æ—§å‰§åœº",
    	"IPTV": "",
    	"PLUS": "+",
    	"ï¼‹": "+",
    	"(": "",
    	")": "",
    	"CAV": "",
    	"ç¾æ´²": "",
    	"åŒ—ç¾": "",
    	"12M": "",
    	"é«˜æ¸…æµ‹è¯•CCTV-1": "",
    	"é«˜æ¸…æµ‹è¯•CCTV-2": "",
    	"é«˜æ¸…æµ‹è¯•CCTV-7": "",
    	"é«˜æ¸…æµ‹è¯•CCTV-10": "",
    	"LD": "",
    	"HEVC20M": "",
    	"S,": ",",
    	"æµ‹è¯•": "",
    	"CCTW": "CCTV",
    	"è¯•çœ‹": "",
    	"æµ‹è¯•": "",
    	" ": "",
    	"æµ‹è¯•cctv": "CCTV",
    	"CCTV1ç»¼åˆ": "CCTV1",
    	"CCTV2è´¢ç»": "CCTV2",
    	"CCTV3ç»¼è‰º": "CCTV3",
    	"CCTV4å›½é™…": "CCTV4",
    	"CCTV4ä¸­æ–‡å›½é™…": "CCTV4",
    	"CCTV4æ¬§æ´²": "CCTV4",
    	"CCTV5ä½“è‚²": "CCTV5",
    	"CCTV5+ä½“è‚²": "CCTV5+",
    	"CCTV6ç”µå½±": "CCTV6",
    	"CCTV7å†›äº‹": "CCTV7",
    	"CCTV7å†›å†œ": "CCTV7",
    	"CCTV7å†œä¸š": "CCTV7",
    	"CCTV7å›½é˜²å†›äº‹": "CCTV7",
    	"CCTV8ç”µè§†å‰§": "CCTV8",
    	"CCTV8å½±è§†": "CCTV8",
    	"CCTV8çºªå½•": "CCTV9",
    	"CCTV9è®°å½•": "CCTV9",
    	"CCTV9çºªå½•": "CCTV9",
    	"CCTV10ç§‘æ•™": "CCTV10",
    	"CCTV11æˆæ›²": "CCTV11",
    	"CCTV12ç¤¾ä¼šä¸æ³•": "CCTV12",
    	"CCTV13æ–°é—»": "CCTV13",
    	"CCTVæ–°é—»": "CCTV13",
    	"CCTV14å°‘å„¿": "CCTV14",
    	"å¤®è§†14å°‘å„¿": "CCTV14",
    	"CCTVå°‘å„¿è¶…": "CCTV14",
    	"CCTV15éŸ³ä¹": "CCTV15",
    	"CCTVéŸ³ä¹": "CCTV15",
    	"CCTV16å¥¥æ—åŒ¹å…‹": "CCTV16",
    	"CCTV17å†œä¸šå†œæ‘": "CCTV17",
    	"CCTV17å†›å†œ": "CCTV17",
    	"CCTV17å†œä¸š": "CCTV17",
    	"CCTV5+ä½“è‚²èµ›è§†": "CCTV5+",
    	"CCTV5+èµ›è§†": "CCTV5+",
    	"CCTV5+ä½“è‚²èµ›äº‹": "CCTV5+",
    	"CCTV5+èµ›äº‹": "CCTV5+",
    	"CCTV5+ä½“è‚²": "CCTV5+",
    	"CCTV5èµ›äº‹": "CCTV5+",
    	"å‡¤å‡°ä¸­æ–‡å°": "å‡¤å‡°ä¸­æ–‡",
    	"å‡¤å‡°èµ„è®¯å°": "å‡¤å‡°èµ„è®¯",
    	"(CCTV4Kæµ‹è¯•ï¼‰": "CCTV4K",
    	"ä¸Šæµ·ä¸œæ–¹å«è§†": "ä¸Šæµ·å«è§†",
    	"ä¸œæ–¹å«è§†": "ä¸Šæµ·å«è§†",
    	"å†…è’™å«è§†": "å†…è’™å¤å«è§†",
    	"ç¦å»ºä¸œå—å«è§†": "ä¸œå—å«è§†",
    	"å¹¿ä¸œå—æ–¹å«è§†": "å—æ–¹å«è§†",
    	"æ¹–å—é‡‘é¹°å¡é€š": "é‡‘é¹°å¡é€š",
    	"ç‚«åŠ¨å¡é€š": "å“ˆå“ˆç‚«åŠ¨",
    	"å¡é…·å¡é€š": "å¡é…·å°‘å„¿",
    	"å¡é…·åŠ¨ç”»": "å¡é…·å°‘å„¿",
    	"BRTVKAKUå°‘å„¿": "å¡é…·å°‘å„¿",
    	"ä¼˜æ›¼å¡é€š": "ä¼˜æ¼«å¡é€š",
    	"ä¼˜æ›¼å¡é€š": "ä¼˜æ¼«å¡é€š",
    	"å˜‰ä½³å¡é€š": "ä½³å˜‰å¡é€š",
    	"ä¸–ç•Œåœ°ç†": "åœ°ç†ä¸–ç•Œ",
    	"CCTVä¸–ç•Œåœ°ç†": "åœ°ç†ä¸–ç•Œ",
    	"BTVåŒ—äº¬å«è§†": "åŒ—äº¬å«è§†",
    	"BTVå†¬å¥¥çºªå®": "å†¬å¥¥çºªå®",
    	"ä¸œå¥¥çºªå®": "å†¬å¥¥çºªå®",
    	"å«è§†å°": "å«è§†",
    	"æ¹–å—ç”µè§†å°": "æ¹–å—å«è§†",
    	"å°‘å„¿ç§‘æ•™": "å°‘å„¿",
    	"å½±è§†å‰§": "å½±è§†",
    	"ç”µè§†å‰§": "å½±è§†",
    	"CCTV1CCTV1": "CCTV1",
    	"CCTV2CCTV2": "CCTV2",
    	"CCTV7CCTV7": "CCTV7",
    	"CCTV10CCTV10": "CCTV10"
}
with open('æ±‡æ€».txt', 'w', encoding='utf-8') as new_file:
    for line in lines:
        # å»é™¤è¡Œå°¾çš„æ¢è¡Œç¬¦
        line = line.rstrip('\n')
        # åˆ†å‰²è¡Œï¼Œè·å–é€—å·å‰çš„å­—ç¬¦ä¸²
        parts = line.split(',', 1)
        if len(parts) > 0:
            # æ›¿æ¢é€—å·å‰çš„å­—ç¬¦ä¸²
            before_comma = parts[0]
            for old, new in replacements.items():
                before_comma = before_comma.replace(old, new)
            # å°†æ›¿æ¢åçš„é€—å·å‰éƒ¨åˆ†å’Œé€—å·åéƒ¨åˆ†é‡æ–°ç»„åˆæˆä¸€è¡Œï¼Œå¹¶å†™å…¥æ–°æ–‡ä»¶
            new_line = f'{before_comma},{parts[1]}\n' if len(parts) > 1 else f'{before_comma}\n'
            new_file.write(new_line)






# æ‰“å¼€æ–‡æœ¬æ–‡ä»¶è¿›è¡Œè¯»å–
def read_and_process_file(input_filename, output_filename, encodings=['utf-8', 'gbk']):
    for encoding in encodings:
        try:
            with open(input_filename, 'r', encoding=encoding) as file:
                lines = file.readlines()
                break
        except UnicodeDecodeError:
            continue
    else:
        raise ValueError(f"Cannot decode file '{input_filename}' with any of the provided encodings")

    with open(output_filename, 'w', encoding='utf-8') as outfile:
        for line in lines:
            if '$' in line:
                processed_line = line.split('$')[0].rstrip('\n')
                outfile.write(processed_line + '\n')
            else:
                outfile.write(line)

# è°ƒç”¨å‡½æ•°
read_and_process_file('æ±‡æ€».txt', 'æ±‡æ€».txt')  # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶åä»¥é¿å…è¦†ç›–åŸå§‹æ–‡ä»¶

###################################################################å»é‡#####################################
def remove_duplicates(input_file, output_file):
    # ç”¨äºå­˜å‚¨å·²ç»é‡åˆ°çš„URLå’ŒåŒ…å«genreçš„è¡Œ
    seen_urls = set()
    seen_lines_with_genre = set()
    # ç”¨äºå­˜å‚¨æœ€ç»ˆè¾“å‡ºçš„è¡Œ
    output_lines = []
    # æ‰“å¼€è¾“å…¥æ–‡ä»¶å¹¶è¯»å–æ‰€æœ‰è¡Œ
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        print("å»é‡å‰çš„è¡Œæ•°ï¼š", len(lines))
        # éå†æ¯ä¸€è¡Œ
        for line in lines:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾URLå’ŒåŒ…å«genreçš„è¡Œ,é»˜è®¤æœ€åä¸€è¡Œ
            urls = re.findall(r'[https]?[http]?[rtsp]?[rtmp]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', line)
            genre_line = re.search(r'\bgenre\b', line, re.IGNORECASE) is not None
            # å¦‚æœæ‰¾åˆ°URLå¹¶ä¸”è¯¥URLå°šæœªè¢«è®°å½•
            if urls and urls[0] not in seen_urls:
                seen_urls.add(urls[0])
                output_lines.append(line)
            # å¦‚æœæ‰¾åˆ°åŒ…å«genreçš„è¡Œï¼Œæ— è®ºæ˜¯å¦å·²è¢«è®°å½•ï¼Œéƒ½å†™å…¥æ–°æ–‡ä»¶
            if genre_line:
                output_lines.append(line)
    # å°†ç»“æœå†™å…¥è¾“å‡ºæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.writelines(output_lines)
    print("å»é‡åçš„è¡Œæ•°ï¼š", len(output_lines))
# ä½¿ç”¨æ–¹æ³•
remove_duplicates('æ±‡æ€».txt', '2.txt')   






######################################################################################æå–goodiptv
import re
import os
# å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¦æ’é™¤çš„å…³é”®è¯çš„åˆ—è¡¨
excluded_keywords = ['epg', 'mitv', 'rtp', 'p3p', 'æ–°é—»ç»¼åˆ', 'P3p', 'jdshipin#', '9930/qilu', 'gitcode.net', '151:99', '21dtv', 'txmov2', 'gcw.bdcdn', 'metshop', 'mp4', 
                     'shandong', 'goodiptv', 'è´­ç‰©', '[', 'P3P', 'è…”', 'æ›²', '//1', 'æ˜¥èŠ‚', 'ç½‘ç»œæ”¶é›†', '95.179', 'hlspull', 'github', 'lunbo', 'tw.ts138', '114:8278', '//tvb', 'extraott',  
                     '22:8891', 'fanmingming', '43:22222', 'etv.xhgvip', 'free.xiptv', 'www.zhixun', 'xg.52sw', 'iptv.yjxfz.com', 'zb.qc', '/vd', '/TV2/']   #, '/TV2/'

# å®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¦æå–çš„å…³é”®è¯çš„åˆ—è¡¨
extract_keywords = ['å‡¤å‡°å«è§†', 'ç¿¡ç¿ å°', 'å‡¤å‡°é¦™æ¸¯', 'å‡¤å‡°ä¸­æ–‡', 'å‡¤å‡°èµ„è®¯', 'ä¸­å¤©æ–°é—»', 'ä¸­å¤©äºšæ´²', 'æ— çº¿æ–°é—»', 'TVB', 'viu', 'ä¸­è§†', 'å…¬è§†', 'å°è§†', 'åè§†', 'CCTV', 'å¹¿å·', 'å¹¿ä¸œ', 'æ–°è§†è§‰', 'CHC', 'è¶³çƒ', 'ä½“è‚²', 'HOY', 'NOW', 'RTHK', 'æ¾³é—¨', 'TVBSæ–°é—»', 'ä¸œæ£®æ–°é—»', 'å¹¿ä¸œå«è§†', 'å¹¿ä¸œä½“è‚²', 'æ·±åœ³å«è§†', 'æ±Ÿè‹å«è§†', 'æ¹–å—å«è§†', 'ä¸œæ–¹å«è§†', 'åŒ—äº¬å«è§†', 'æµ™æ±Ÿå«è§†', 'é’æµ·å«è§†', 'ä¸œå—å«è§†', 'å¤©æ´¥å«è§†', 'è¾½å®å«è§†', 'å››å·å«è§†', 'æ±Ÿè¥¿å«è§†', 'è´µå·å«è§†', 'æ¹–åŒ—å«è§†', 'é‡åº†å«è§†', 'æ²³åŒ—å«è§†', 'ç”˜è‚ƒå«è§†', 'äº‘å—å«è§†', 'å‰æ—å«è§†', 'æ²³å—å«è§†', 'å±±ä¸œå«è§†', 'é‡‘é¹°å¡é€š', 'å®‰å¾½å«è§†', 'å¹¿è¥¿å«è§†', 'æµ·å—å«è§†', 'å¤§æ¹¾åŒºå«è§†', 'CNN', 'ABC', 'Global News', 'FOX', 'BBC']


# è¯»å–æ–‡ä»¶å¹¶å¤„ç†æ¯ä¸€è¡Œ
with open('2.txt', 'r', encoding='utf-8') as file:
    lines = file.readlines()

    # åˆ›å»ºæˆ–æ‰“å¼€ä¸€ä¸ªè¾“å‡ºæ–‡ä»¶ç”¨äºå†™å…¥å¤„ç†åçš„æ•°æ®
    with open('zby2.txt', 'w', encoding='utf-8') as outfile:
        for line in lines:
            # é¦–å…ˆæ£€æŸ¥è¡Œæ˜¯å¦åŒ…å«ä»»ä½•æå–å…³é”®è¯
            if any(keyword in line for keyword in extract_keywords):
                # å¦‚æœåŒ…å«æå–å…³é”®è¯ï¼Œè¿›ä¸€æ­¥æ£€æŸ¥è¡Œæ˜¯å¦ä¸åŒ…å«ä»»ä½•æ’é™¤å…³é”®è¯
                if not any(keyword in line for keyword in excluded_keywords):
                    outfile.write(line)  # å†™å…¥ç¬¦åˆæ¡ä»¶çš„è¡Œåˆ°æ–‡ä»¶


###############################################################
import re
def parse_file(input_file_path, output_file_name):
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»'//'å¼€å§‹åˆ°ç¬¬ä¸€ä¸ª'/'æˆ–ç¬¬ä¸€ä¸ª'::'ç»“æŸçš„éƒ¨åˆ†
    ip_or_domain_pattern = re.compile(r'//([^/:]*:[^/:]*::[^/:]*|[^/]*)')
    # ç”¨äºå­˜å‚¨æ¯ä¸ªIPæˆ–åŸŸååŠå…¶å¯¹åº”çš„è¡Œåˆ—è¡¨
    ip_or_domain_to_lines = {}
    # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
    with open(input_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            line = line.strip()
            # å¦‚æœè¡Œæ˜¯åˆ†ç±»æ ‡ç­¾è¡Œï¼Œåˆ™è·³è¿‡
            if ",#genre#" in line:
                continue
            # æ£€æŸ¥è¡Œæ˜¯å¦åŒ…å«IPæˆ–åŸŸå
            match = ip_or_domain_pattern.search(line)
            if match:
                # æå–åŒ¹é…åˆ°çš„IPæˆ–åŸŸå
                matched_text = match.group(1)
                # å»é™¤IPæˆ–åŸŸååçš„å‰©ä½™éƒ¨åˆ†ï¼Œåªä¿ç•™åŒ¹é…åˆ°çš„IPæˆ–åŸŸå
                ip_or_domain = matched_text.split('://')[-1].split('/')[0].split('::')[0]
                # å°†è¡Œæ·»åŠ åˆ°å¯¹åº”çš„IPæˆ–åŸŸååˆ—è¡¨ä¸­
                if ip_or_domain not in ip_or_domain_to_lines:
                    ip_or_domain_to_lines[ip_or_domain] = []
                ip_or_domain_to_lines[ip_or_domain].append(line)
    ############################################################################### è¿‡æ»¤æ‰å°äº1500å­—èŠ‚çš„IPæˆ–åŸŸåæ®µ
    filtered_ip_or_domain_to_lines = {ip_or_domain: lines for ip_or_domain, lines in ip_or_domain_to_lines.items()
                                      if sum(len(line) for line in lines) >= 300}
    # å¦‚æœæ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„IPæˆ–åŸŸåæ®µï¼Œåˆ™ä¸ç”Ÿæˆæ–‡ä»¶
    if not filtered_ip_or_domain_to_lines:
        print("æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„IPæˆ–åŸŸåæ®µï¼Œä¸ç”Ÿæˆæ–‡ä»¶ã€‚")
        return
    # åˆå¹¶æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„IPæˆ–åŸŸåçš„è¡Œåˆ°ä¸€ä¸ªæ–‡ä»¶
    with open(output_file_name, 'w', encoding='utf-8') as output_file:
        for ip_or_domain, lines in filtered_ip_or_domain_to_lines.items():
            # å†™å…¥IPæˆ–åŸŸååŠå…¶å¯¹åº”çš„è¡Œåˆ°è¾“å‡ºæ–‡ä»¶
            output_file.write(f"é¢‘é“,#genre#\n")
            for line in lines:
                output_file.write(line + '\n')
            output_file.write('\n')  # åœ¨æ¯ä¸ªå°æ®µåæ·»åŠ ä¸€ä¸ªç©ºè¡Œä½œä¸ºåˆ†éš”
# è°ƒç”¨å‡½æ•°å¹¶ä¼ å…¥æ–‡ä»¶è·¯å¾„å’Œè¾“å‡ºæ–‡ä»¶å
parse_file('zby2.txt', 'zby2.txt')


import cv2
import time
from tqdm import tqdm
import os

# å­˜å‚¨æ–‡ä»¶è·¯å¾„
file_path = "zby2.txt"
output_file_path = "zby2.txt"

def get_ip_key(url):
    """ä» URL ä¸­æå– IP åœ°å€ï¼Œå¹¶æ„é€ ä¸€ä¸ªå”¯ä¸€çš„é”®"""
    start = url.find('://') + 3
    end = url.find('/', start)
    return url[start:end] if end!= -1 else None

def merge_and_filter():
    with open(file_path, 'r', encoding='utf-8') as file:
        lines = file.readlines()

    total_lines = len(lines)

    # å¤„ç†è¾“å…¥æ–‡ä»¶ä¸­çš„æ•°æ®å¹¶è¿›è¡Œæ£€æµ‹
    with open(output_file_path, 'a', encoding='utf-8') as output_file:
        for i, line in tqdm(enumerate(lines), total=total_lines, desc="Processing", unit='line'):
            if 'genre' in line:
                output_file.write(line)
                continue
            parts = line.split(',', 1)
            if len(parts) == 2:
                channel_name, url = parts
                channel_name = channel_name.strip()
                url = url.strip()
                ip_key = get_ip_key(url)
                if ip_key and ip_key in detected_ips:
                    if detected_ips[ip_key]['status'] == 'ok':
                        output_file.write(line)
                elif ip_key:
                    cap = cv2.VideoCapture(url)
                    start_time = time.time()
                    frame_count = 0
                    while frame_count < 50 and (time.time() - start_time) < 3:
                        ret, frame = cap.read()
                        if not ret:
                            break
                        frame_count += 1
                    cap.release()
                    if frame_count >= 50:
                        detected_ips[ip_key] = {'status': 'ok'}
                        output_file.write(line)
                    else:
                        detected_ips[ip_key] = {'status': 'fail'}

    # åˆå¹¶ä»»æ„å­—ç¬¦åŠ ä¸Šzby2.txt çš„æ–‡ä»¶
    all_files = [f for f in os.listdir(os.getcwd()) if f.endswith('zby2.txt')]
    with open(output_file_path, 'a', encoding='utf-8') as master_output:
        for file_name in all_files:
            if file_name!= output_file_path:
                with open(file_name, 'r', encoding='utf-8') as other_file:
                    content = other_file.read()
                    if content:
                        master_output.write('\n')
                        master_output.write(content)

detected_ips = {}
merge_and_filter()

for ip_key, result in detected_ips.items():
    print(f"IP Key: {ip_key}, Status: {result['status']}")



def remove_duplicates(input_file, output_file):
    # ç”¨äºå­˜å‚¨å·²ç»é‡åˆ°çš„URLå’ŒåŒ…å«genreçš„è¡Œ
    seen_urls = set()
    seen_lines_with_genre = set()
    # ç”¨äºå­˜å‚¨æœ€ç»ˆè¾“å‡ºçš„è¡Œ
    output_lines = []
    # æ‰“å¼€è¾“å…¥æ–‡ä»¶å¹¶è¯»å–æ‰€æœ‰è¡Œ
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        print("å»é‡å‰çš„è¡Œæ•°ï¼š", len(lines))
        # éå†æ¯ä¸€è¡Œ
        for line in lines:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾URLå’ŒåŒ…å«genreçš„è¡Œ,é»˜è®¤æœ€åä¸€è¡Œ
            urls = re.findall(r'[https]?[http]?[P2p]?[mitv]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', line)
            genre_line = re.search(r'\bgenre\b', line, re.IGNORECASE) is not None
            # å¦‚æœæ‰¾åˆ°URLå¹¶ä¸”è¯¥URLå°šæœªè¢«è®°å½•
            if urls and urls[0] not in seen_urls:
                seen_urls.add(urls[0])
                output_lines.append(line)
            # å¦‚æœæ‰¾åˆ°åŒ…å«genreçš„è¡Œ,æ— è®ºæ˜¯å¦å·²è¢«è®°å½•,éƒ½å†™å…¥æ–°æ–‡ä»¶
            if genre_line:
                output_lines.append(line)
    # å°†ç»“æœå†™å…¥è¾“å‡ºæ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.writelines(output_lines)
    print("å»é‡åçš„è¡Œæ•°ï¼š", len(output_lines))
# ä½¿ç”¨æ–¹æ³•
remove_duplicates('zby2.txt', 'zby2.txt')





######################è¿é€šæ€§æ£€æµ‹

import requests
import time
import cv2
from urllib.parse import urlparse
from tqdm import tqdm

# æµ‹è¯•HTTPè¿æ¥å¹¶å°è¯•ä¸‹è½½æ•°æ®
def test_connectivity_and_download(url, initial_timeout=1, retry_timeout=1):
    parsed_url = urlparse(url)
    if parsed_url.scheme not in ['http', 'https']:
        # éHTTP(s)åè®®ï¼Œå°è¯•RTSPæ£€æµ‹
        return test_rtsp_connectivity(url, retry_timeout)
    else:
        # HTTP(s)åè®®ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        try:
            with requests.get(url, stream=True, timeout=initial_timeout) as response:
                if response.status_code == 200:
                    start_time = time.time()
                    while time.time() - start_time < initial_timeout:
                        chunk = response.raw.read(51200)  # å°è¯•ä¸‹è½½1KBæ•°æ®
                        if chunk:
                            return True  # æˆåŠŸä¸‹è½½æ•°æ®
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {e}")
            pass #è¿™è¡Œåˆ æ‰åˆ™ä¼šåœ¨ä¸‹è½½ä¸åˆ°æ•°æ®æµçš„æ—¶å€™è¿›è¡Œè¿é€šæ€§æµ‹è¯•

    return False  # é»˜è®¤è¿”å›False

print("/" * 80)

# æµ‹è¯•RTSPè¿æ¥å¹¶å°è¯•è¯»å–æµ
def test_rtsp_connectivity(url, timeout=3):
    cap = cv2.VideoCapture(url)
    if not cap.isOpened():
        return False
    start_time = time.time()
    while time.time() - start_time < timeout:
        ret, _ = cap.read()
        if ret:
            return True  # æˆåŠŸè¯»å–å¸§
    cap.release()
    return False

# ä¸»å‡½æ•°
def master(è¾“å…¥, è¾“å‡º):
    with open(è¾“å…¥, "r", encoding="utf-8") as source_file:
        lines = source_file.readlines()

    results = []
    for line_number, line in enumerate(tqdm(lines, desc="æ£€æµ‹ä¸­")):
        parts = line.strip().split(",")
        if len(parts) == 2 and parts[1]:  # ç¡®ä¿æœ‰URLï¼Œå¹¶ä¸”URLä¸ä¸ºç©º
            channel_name, channel_url = parts
            try:
                is_valid = test_connectivity_and_download(channel_url)
            except Exception as e:
                print(f"æ£€æµ‹URL {channel_url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                is_valid = False  # å°†å¼‚å¸¸çš„URLè§†ä¸ºæ— æ•ˆ

            status = "æœ‰æ•ˆ" if is_valid else "æ— æ•ˆ"

            if "genre" in line.lower() or status == "æœ‰æ•ˆ":
                results.append((channel_name.strip(), channel_url.strip(), status))

    # å†™å…¥æ–‡ä»¶
    with open(è¾“å‡º, "w", encoding="utf-8") as output_file:
        for channel_name, channel_url, status in results:
            output_file.write(f"{channel_name},{channel_url}\n")

    print(f"ä»»åŠ¡å®Œæˆ, æœ‰æ•ˆæºæ•°é‡: {len([x for x in results if x[2] == 'æœ‰æ•ˆ'])}, æ— æ•ˆæºæ•°é‡: {len([x for x in results if x[2] == 'æ— æ•ˆ'])}")

if __name__ == "__master__":
    è¾“å…¥ =  "zby2.txt"    #input('è¯·è¾“å…¥utf-8ç¼–ç çš„ç›´æ’­æºæ–‡ä»¶è·¯å¾„:')
    è¾“å‡º = "zby2.txt"
    master(è¾“å…¥, è¾“å‡º)




import re
from pypinyin import lazy_pinyin
# æ‰“å¼€ä¸€ä¸ªutf-8ç¼–ç çš„æ–‡æœ¬æ–‡ä»¶
with open("zby2.txt", "r", encoding="utf-8") as file:
    # è¯»å–æ‰€æœ‰è¡Œå¹¶å­˜å‚¨åˆ°åˆ—è¡¨ä¸­
    lines = file.readlines()
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæå–æ¯è¡Œçš„ç¬¬ä¸€ä¸ªæ•°å­—
def extract_first_number(line):
    match = re.search(r'\d+', line)
    return int(match.group()) if match else float('inf')
# å¯¹åˆ—è¡¨ä¸­çš„è¡Œè¿›è¡Œæ’åºï¼ŒæŒ‰ç…§ç¬¬ä¸€ä¸ªæ•°å­—çš„å¤§å°æ’åˆ—ï¼Œå…¶ä½™è¡ŒæŒ‰ä¸­æ–‡æ’åº
sorted_lines = sorted(lines, key=lambda x: (not 'CCTV' in x, extract_first_number(x) if 'CCTV' in x else lazy_pinyin(x.strip())))
# å°†æ’åºåçš„è¡Œå†™å…¥æ–°çš„utf-8ç¼–ç çš„æ–‡æœ¬æ–‡ä»¶
with open("zby2.txt", "w", encoding="utf-8") as file:
    for line in sorted_lines:
        file.write(line)




def parse_file(input_file_path, output_file_name):    #
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»'//'å¼€å§‹åˆ°ç¬¬ä¸€ä¸ª'/'æˆ–ç¬¬ä¸€ä¸ª'::'ç»“æŸçš„éƒ¨åˆ†
    ip_or_domain_pattern = re.compile(r'//([^/:]*:[^/:]*::[^/:]*|[^/]*)')
    # ç”¨äºå­˜å‚¨æ¯ä¸ªIPæˆ–åŸŸååŠå…¶å¯¹åº”çš„è¡Œåˆ—è¡¨
    ip_or_domain_to_lines = {}
    # ç”¨äºç”Ÿæˆåˆ†ç±»åçš„å­—æ¯å’Œæ•°å­—è®¡æ•°å™¨
    alphabet_counter = 0  # å­—æ¯è®¡æ•°å™¨ï¼Œä»0å¼€å§‹
    number_counter = 1     # æ•°å­—è®¡æ•°å™¨ï¼Œä»1å¼€å§‹
    # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
    with open(input_file_path, 'r', encoding='utf-8') as file:
        for line in file:
            line = line.strip()
            # å¦‚æœè¡Œæ˜¯åˆ†ç±»æ ‡ç­¾è¡Œï¼Œåˆ™è·³è¿‡
            if ",#genre#" in line:
                continue
            # æ£€æŸ¥è¡Œæ˜¯å¦åŒ…å«IPæˆ–åŸŸå
            match = ip_or_domain_pattern.search(line)
            if match:
                # æå–åŒ¹é…åˆ°çš„IPæˆ–åŸŸå
                matched_text = match.group(1)
                # å»é™¤IPæˆ–åŸŸååçš„å‰©ä½™éƒ¨åˆ†ï¼Œåªä¿ç•™åŒ¹é…åˆ°çš„IPæˆ–åŸŸå
                ip_or_domain = matched_text.split('://')[-1].split('/')[0].split('::')[0]
                # å°†è¡Œæ·»åŠ åˆ°å¯¹åº”çš„IPæˆ–åŸŸååˆ—è¡¨ä¸­
                if ip_or_domain not in ip_or_domain_to_lines:
                    ip_or_domain_to_lines[ip_or_domain] = []
                ip_or_domain_to_lines[ip_or_domain].append(line)
    # è¿‡æ»¤æ‰å°äº1000å­—èŠ‚çš„IPæˆ–åŸŸåæ®µ
    filtered_ip_or_domain_to_lines = {ip_or_domain: lines for ip_or_domain, lines in ip_or_domain_to_lines.items()
                                      if sum(len(line) for line in lines) >= 250}   # è¿‡æ»¤æ‰å°äº1000å­—èŠ‚çš„IPæˆ–åŸŸåæ®µ
    # å¦‚æœæ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„IPæˆ–åŸŸåæ®µï¼Œåˆ™ä¸ç”Ÿæˆæ–‡ä»¶
    if not filtered_ip_or_domain_to_lines:
        print("æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„IPæˆ–åŸŸåæ®µï¼Œä¸ç”Ÿæˆæ–‡ä»¶ã€‚")
        return
    # åˆå¹¶æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„IPæˆ–åŸŸåçš„è¡Œåˆ°ä¸€ä¸ªæ–‡ä»¶
############################################################
    with open(output_file_name, 'w', encoding='utf-8') as output_file:   #output_
        for ip_or_domain, lines in filtered_ip_or_domain_to_lines.items():
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é€’å¢æ•°å­—è®¡æ•°å™¨
            if alphabet_counter >= 26:
                number_counter += 1
                alphabet_counter = 0  # é‡ç½®å­—æ¯è®¡æ•°å™¨          
 ######################################################              
            # ç”Ÿæˆåˆ†ç±»å

# è¯»å–åŸå§‹æ–‡æœ¬æ–‡ä»¶
with open('zby2.txt', 'r', encoding='utf-8') as file:
       lines = file.readlines()

# å®šä¹‰å­˜å‚¨é¢‘é“ä¿¡æ¯çš„å­—å…¸
channels = {
         "ğŸ’å¸¸çœ‹": [],
         "â›¹ï¸ä½“è‚²": [],
         "ğŸ‡­ğŸ‡°æ¸¯å°": [],
         "ğŸ‡¨ğŸ‡³å¤®è§†": [],
         "ğŸ“¡å«è§†": [],
         "ğŸŒå¤–å›½": [],
         "ğŸ“¹å…¶å®ƒ": []
}

# éå†æ¯ä¸€è¡Œ
for line in lines:
      line_lower = line.lower()   #å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ä»¥ä¾¿è¿›è¡ŒåŒ¹é…
      line=line.replace(': ',' , ')
      if "å‡¤å‡°" in line_lower or "ç¿¡ç¿ å°" in line_lower or "æ— çº¿æ–°é—»" in line_lower or "å¹¿å·" in line_lower or "å¹¿ä¸œç æ±Ÿ" in line_lower:
         channels["ğŸ’å¸¸çœ‹"].append(line.strip())
      elif "cctv5" in line_lower or "cctv5+" in line_lower or "cctv16" in line_lower or "ä½“è‚²" in line_lower  or "è¶³çƒ" in line_lower or "ç«èµ›" in line_lower:
         channels["â›¹ï¸ä½“è‚²"].append(line.strip()) 
      elif "ä¸­å¤©" in line_lower or "ä¸­è§†" in line_lower or "å°è§†" in line_lower or "åè§†" in line_lower  or "hoy" in line_lower or "rthk" in line_lower or "now" in line_lower or "tvb" in line_lower or "viu" in line_lower or "tvbsæ–°é—»" in line_lower or "ä¸œæ£®æ–°é—»" in line_lower or "å¯°å®‡æ–°é—»" in line_lower:
         channels["ğŸ‡­ğŸ‡°æ¸¯å°"].append(line.strip()) 
      elif "cctv" in line_lower:
          channels["ğŸ‡¨ğŸ‡³å¤®è§†"].append(line.strip())
      elif "å«è§†" in line_lower:
          channels["ğŸ“¡å«è§†"].append(line.strip())
      elif "cnn" in line_lower or "global news" in line_lower or "bbc news" in line_lower or "fox news" in line_lower  or "abc news" in line_lower:
          channels["ğŸŒå¤–å›½"].append(line.strip())
      else:
          channels["ğŸ“¹å…¶å®ƒ"].append(line.strip())

#å†™å…¥æ–°çš„æ–‡æœ¬æ–‡ä»¶
with open('zby2.txt', 'w', encoding='utf-8') as file:
       for genre, channel_list in channels.items():
            file.write(f"{genre},#genre#\n")
            for channel in channel_list:
                  file.write(f"{channel}\n")
            file.write('\n')
print("æ‰§è¡Œå®Œæ¯•")


# æ‰“å¼€å¹¶è¯»å– local.txt æ–‡ä»¶
with open('local.txt', 'r', encoding='utf-8') as local_file:
       local_content = local_file.read()
 
# æ‰“å¼€ zby2.txt æ–‡ä»¶ï¼Œå¹¶å°† local.txt çš„å†…å®¹è¿½åŠ åˆ° zby2.txt çš„æœ«å°¾
with open('zby2.txt', 'a', encoding='utf-8') as zby2_file:
       zby2_file.write(local_content)

import datetime
now = datetime.datetime.utcnow() + datetime.timedelta(hours=8)
current_time = now.strftime("%Y/%m/%d %H:%M")
# ä»¥è¿½åŠ æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶åœ¨æœ«å°¾æ·»åŠ æ–°è¡Œå’Œæ—¶é—´
with open('zby2.txt', 'a') as file:
    file.write(f"\n{current_time}æ›´æ–°")  # ç¡®ä¿æ—¶é—´åœ¨æ–°çš„ä¸€è¡Œ       

################################################################################################ä»»åŠ¡ç»“æŸï¼Œåˆ é™¤ä¸å¿…è¦çš„è¿‡ç¨‹æ–‡ä»¶
files_to_remove = ["2.txt", "æ±‡æ€».txt"]
for file in files_to_remove:
    if os.path.exists(file):
        os.remove(file)
    else:              # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™æç¤ºå¼‚å¸¸å¹¶æ‰“å°æç¤ºä¿¡æ¯
        print(f"æ–‡ä»¶ {file} ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤ã€‚")
print("ä»»åŠ¡è¿è¡Œå®Œæ¯•ï¼Œé¢‘é“åˆ—è¡¨å¯æŸ¥çœ‹ä»“åº“zby2.txtæ–‡ä»¶ï¼")
